{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJRhl22NdNFA"
      },
      "source": [
        "## Training a character language model and studying various ways of generating text\n",
        "\n",
        "**Author: Souhail OUMAMA (souhail.oumama@telecom-paris.fr)**  \n",
        "**Based on Original lab by : Matthieu LABEAU (matthieu.labeau@telecom-paris.fr)**\n",
        "\n",
        "## Objectives:\n",
        "\n",
        "- We will train a network to predict a next character given an input sequence of characters, and use it to generate new sequences.\n",
        "- We will strictly work with local (and not structured) prediction - however, we will look into a relatively simple heuristic to improve the \"structure\": *beam search*. We can also try to improve generation with other methods: *temperature* sampling, *top-k* sampling, *top-p* sampling,\n",
        "- We will use ```keras```to build the model based on a LSTM, which will use simple features (one-hot vector representing previous characters) to predict the next characters. We will use a small model to avoid training for too long.\n",
        "- We will use a small dataset (poetry, from project Gutenberg) - you can use any data you prefer, as long as you are able to train the model on it. \n",
        "- Even with a small dataset and a small model, training may be long. If you can use a computing infrastructure, like Google colab, it may be more practical - and you probably can obtain better results by using a bigger model and a larger dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JfjG_pve1Ds"
      },
      "source": [
        "#### Obtaining the data \n",
        "- We download directly the ebook from project Gutenberg - you can get any other text you would prefer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pXPnJoA6fTe",
        "outputId": "33aeed26-be36-4cde-9641-269e47b7bc2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "URL transformed to HTTPS due to an HSTS policy\n",
            "--2021-11-30 22:29:03--  https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4573338 (4.4M) [text/plain]\n",
            "Saving to: ‘shakespeare_input.txt.3’\n",
            "\n",
            "shakespeare_input.t 100%[===================>]   4.36M  3.70MB/s    in 1.2s    \n",
            "\n",
            "2021-11-30 22:29:04 (3.70 MB/s) - ‘shakespeare_input.txt.3’ saved [4573338/4573338]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BCd_FKg1chKU"
      },
      "outputs": [],
      "source": [
        "from keras.utils.data_utils import get_file\n",
        "url = 'http://www.gutenberg.org/cache/epub/6099/pg6099.txt'\n",
        "path = get_file('pg6099.txt', origin=url)\n",
        "\n",
        "f = open(path, 'r' , encoding = 'utf8')\n",
        "lines = f.readlines()\n",
        "text = []\n",
        "\n",
        "start = False\n",
        "for line in lines:\n",
        "    if(\"START OF THIS PROJECT GUTENBERG EBOOK LES FLEURS DU MAL\" in line and start==False):\n",
        "        start = True\n",
        "    if(\"END OF THIS PROJECT GUTENBERG EBOOK LES FLEURS DU MAL\" in line):\n",
        "        break\n",
        "    if(start==False or len(line) == 0):\n",
        "        continue\n",
        "    text.append(line)\n",
        "\n",
        "f.close()\n",
        "text = \" \".join(text)\n",
        "voc_chars = sorted(set([c for c in text]))\n",
        "nb_chars = len(voc_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrHl4Y3SEeWB",
        "outputId": "43c97f0d-c339-4967-b8cb-9ad832c328b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ces plaintes,\n",
            "   Ces extases, ces cris, ces pleurs, ces _Te Deum,_\n",
            "   Sont un écho redit par mille labyrinthes;\n",
            "   C'est pour les coeurs mortels un divin opium.\n",
            " \n",
            "   C'est un cri répété par mille sentinelles,\n",
            "   Un ordre renvoyé par mille porte-voix;\n",
            "   C'est un phare allumé sur mille citadelles,\n",
            "   Un appel de chasseurs perdus dans les grands bois!\n",
            " \n",
            "   Car c'est vraiment, Seigneur, le meilleur témoignage\n",
            "   Que nous puissions donner de notre dignité\n",
            "   Que cet ardent sanglot qui roule d'âge en âge\n",
            "   Et vient mourir au bord de votre éternité!\n",
            " \n",
            " \n",
            " \n",
            " \n",
            "   LA MUSE VENALE\n",
            " \n",
            " \n",
            "   O Muse de mon coeur, amante des palais,\n",
            "   Auras-tu, quand Janvier lâchera ses Borées,\n",
            "   Durant les noirs ennuis des neigeuses soirées,\n",
            "   Un tison pour chauffer tes deux pieds violets?\n",
            " \n",
            "   Ranimeras-tu donc tes épaules marbrées\n",
            "   Aux nocturnes rayons qui percent les volets?\n",
            "   Sentant ta bourse à sec autant que ton palais,\n",
            "   Récolteras-tu l'or des voûtes azurées?\n",
            " \n",
            "   Il te faut, pour gagner ton pain de cha\n"
          ]
        }
      ],
      "source": [
        "print(text[20000:21000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SN-Cq6PGe61b"
      },
      "source": [
        "#### Keeping track of possible characters\n",
        "- Using a ```set```, create a sorted list of possible characters\n",
        "- Create two dictionnaries, having characters and corresponding indexes as {key: value}, and reverse.\n",
        "\n",
        "Example:\n",
        "\n",
        "```chars = [a, b, c]```\n",
        "\n",
        "```chars_indices = {a: 0, b: 1, c: 2}```\n",
        "\n",
        "```indices_chars = {0: a, 1: b, 2: c}```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdbvrYqHe9cg",
        "outputId": "4fc5def3-bddd-43c2-b6e7-a7f2f4a9cef2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus length: 160576\n",
            "Total number of characters: 92\n"
          ]
        }
      ],
      "source": [
        "print('Corpus length:', len(text))\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "print('Total number of characters:', len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzqwN_5gfEcQ"
      },
      "source": [
        "#### Creating training data\n",
        "- We will represent characters using *one-hot vectors*. Hence, the i-th character of n possible characters will be represented by a vector of length $n$, containing $0$ expect for a $1$ in position $i$. Following our previous examples, ```a = [1, 0, 0]``` and ```b = [0, 1, 0]```. \n",
        "- Hence, a sequence of characters is a list of one-hot vectors. Our goal will be to predict, given an input sequence of fixed length (here, this length is given by ```maximum_seq_length```) the next character. Hence, we need to build two lists: ```sentences```, containing the input sequences, and ```next_char``` the characters to be predicted. \n",
        "- We do not necessarily need to take all possible sequences. We can select one every ```time_step``` steps.\n",
        "\n",
        "Example: Using the previous dictionnaries, the sequence:\n",
        "```'acabbaccaabba'``` with ```maximum_seq_length = 4``` and ```time_step = 2``` would give the following lists: \n",
        "\n",
        "```sentences = ['acab', 'abba', 'bacc', 'ccaa', 'aabb']```\n",
        "\n",
        "```next_char = ['b', 'c', 'a', 'b', 'a']```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIIvHDZvfHn7",
        "outputId": "0814267f-8db2-4ab2-f5e0-28f22c1e277e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Sequences: 160552\n"
          ]
        }
      ],
      "source": [
        "maximum_seq_length = 24\n",
        "time_step = 1\n",
        "sentences = []\n",
        "next_char = []\n",
        "for i in range(0, len(text) - maximum_seq_length, time_step):\n",
        "    sentences.append(text[i: i + maximum_seq_length])\n",
        "    next_char.append(text[i + maximum_seq_length])\n",
        "print('Number of Sequences:', len(sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UYa-53Qqfcac"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7z7a6_7femM"
      },
      "source": [
        "#### Creating training tensors\n",
        "- We need to transform these lists into tensors, using one-hot vectors to represent characters.\n",
        "- We will need 3 dimensions for the training examples from ```sentences```: the number of examples, the length of the sequence, and the dimension of the one-hot vector\n",
        "- This is reduced to 2 dimensions for the ```next_char```: number of examples and one-hot vector. \n",
        "\n",
        "Example: the previous ```sentences``` would become:\n",
        "\n",
        "```X = [[[1, 0, 0],\n",
        "       [0, 0, 1],\n",
        "       [1, 0, 0],\n",
        "       [0, 1, 0]],\n",
        "      [[1, 0, 0],\n",
        "       [0, 1, 0],\n",
        "       [0, 1, 0],\n",
        "       [1, 0, 0]],\n",
        "      [[0, 1, 0],\n",
        "       [1, 0, 0],\n",
        "       [0, 0, 1],\n",
        "       [0, 0, 1]],\n",
        "      [[0, 0, 1],\n",
        "       [0, 0, 1],\n",
        "       [1, 0, 0],\n",
        "       [1, 0, 0]],\n",
        "      [[1, 0, 0],\n",
        "       [1, 0, 0],\n",
        "       [0, 1, 0],\n",
        "       [0, 1, 0]]] ```\n",
        "       \n",
        "```y = [[0, 1, 0],\n",
        "      [0, 0, 1],\n",
        "      [1, 0, 0],\n",
        "      [0, 1, 0],\n",
        "      [1, 0, 0]]```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oPQEWEKnfgAD"
      },
      "outputs": [],
      "source": [
        "X = np.zeros((len(sentences), maximum_seq_length, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        X[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_char[i]]] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC6jUjYYjPHj"
      },
      "source": [
        "#### Implement the model\n",
        "- We need to create a LSTM model that takes directly out inputs from ```X``` and try to predict one-hot vectors from ```y```.\n",
        "- The model should be made with a LSTM layer, and a Dense layer followed by a softmax activation function. \n",
        "- The most important arguments are the ```input_shape``` for the LSTM layer and the proper size for the Dense layer. How do we find them out ? \n",
        "- We can use a moderate size for the hidden states for the LSTM. \n",
        "- We use the ```categorical_crossentropy``` loss, with the optimizer of our preference (for example, ```adam```)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gGIn8bKEfgGZ"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import LambdaCallback, EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gzIRMSx1gJRh"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(maximum_seq_length, len(chars))))\n",
        "model.add(Dense(len(chars)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XguFAVGjnVG"
      },
      "source": [
        "#### Create a function to generate text with our model\n",
        "- We use the output of our model (a vector of probabilities on characters) to select the next most probable character (with the ```argmax``` function)\n",
        "- We need to transform an input text into an input tensor, as before (taking the right length, the last ```maximum_seq_length``` characters)\n",
        "- We need to transform back the most probable index into a character and add it to our text.\n",
        "- This must be looped ```num_generated``` times, each time obtaining a new input tensor from the new input sequence (which has the character we previously predicted at the end !)\n",
        "\n",
        "The following function (```end_epoch_generate```) is here to facilitate automatic generation at the end of each epoch, so you can monitor of generation changes as the model trains. It calls the ```generate_next``` function upon each sequence of text in ```texts_ex```. The only element in this list right now comes from the training data - you can add your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "C-U3odSfkS70"
      },
      "outputs": [],
      "source": [
        "def generate_next(model, text, num_generated=120):\n",
        "    generated = text\n",
        "    sentence = text[-maximum_seq_length:]\n",
        "    for i in range(num_generated):\n",
        "        x = np.zeros((1, maximum_seq_length, len(chars)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x[0, t, char_indices[char]] = 1.\n",
        "        predictions = model.predict(x, verbose=0)[0]\n",
        "        next_index = np.argmax(predictions)\n",
        "        next_char = indices_char[next_index]\n",
        "        generated += next_char\n",
        "        sentence = sentence[1:] + next_char\n",
        "    return(generated)\n",
        "    \n",
        "def end_epoch_generate(epoch, _):\n",
        "    print('\\n Generating text after epoch: %d' % (epoch+1))\n",
        "    texts_ex = [\"La sottise, l'erreur, le péché\"]\n",
        "    for text in texts_ex:\n",
        "        sample = generate_next(model, text)\n",
        "        print('%s' % (sample))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yC7yZ4LZkTBj",
        "outputId": "b50a2643-3910-47c0-cbb4-35dc958ba41e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"La sottise, l'erreur, le péchéBBtYYYUF__F__7bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\""
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_ex = \"La sottise, l'erreur, le péché\"\n",
        "generate_next(model, text_ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7hDj4zvkTEv",
        "outputId": "fa233828-96e4-47de-a79b-a3c969cc976b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1002/1004 [============================>.] - ETA: 0s - loss: 2.5601\n",
            " Generating text after epoch: 1\n",
            "La sottise, l'erreur, le péchés de pais de pais de pais de pais de pais de pais de pais de pais de pais de pais de pais de pais de pais de pais de pai\n",
            "1004/1004 [==============================] - 24s 21ms/step - loss: 2.5594 - val_loss: 2.1404\n",
            "Epoch 2/10\n",
            "1001/1004 [============================>.] - ETA: 0s - loss: 2.0649\n",
            " Generating text after epoch: 2\n",
            "La sottise, l'erreur, le péchés de l'ant le son son son son son son son son son son son son son son son son son son son son son son son son son son so\n",
            "1004/1004 [==============================] - 21s 21ms/step - loss: 2.0648 - val_loss: 1.9828\n",
            "Epoch 3/10\n",
            "1003/1004 [============================>.] - ETA: 0s - loss: 1.9400\n",
            " Generating text after epoch: 3\n",
            "La sottise, l'erreur, le péchérau,\n",
            "      autes de le fours de  leurs de les den  de les des de le foure de l'un de le des de les des des de le foure d\n",
            "1004/1004 [==============================] - 21s 20ms/step - loss: 1.9402 - val_loss: 2.7869\n",
            "Epoch 4/10\n",
            "1001/1004 [============================>.] - ETA: 0s - loss: 1.8578\n",
            " Generating text after epoch: 4\n",
            "La sottise, l'erreur, le péchés de le pares de le pares de le pares de le pares de le pares de le pares de le pares de le pares de le pares de le pare\n",
            "1004/1004 [==============================] - 20s 20ms/step - loss: 1.8578 - val_loss: 1.8178\n",
            "Epoch 5/10\n",
            "1001/1004 [============================>.] - ETA: 0s - loss: 1.7839\n",
            " Generating text after epoch: 5\n",
            "La sottise, l'erreur, le péchés de ma pleur de ma pleur de ma pleur de ma pleur de ma pleur de ma pleur de ma pleur de ma pleur de ma pleur de ma pleu\n",
            "1004/1004 [==============================] - 21s 21ms/step - loss: 1.7838 - val_loss: 1.7708\n",
            "Epoch 6/10\n",
            "1002/1004 [============================>.] - ETA: 0s - loss: 1.7242\n",
            " Generating text after epoch: 6\n",
            "La sottise, l'erreur, le péchés de la grande et de conte de la grande et de conte de la grande et de conte de la grande et de conte de la grande et de\n",
            "1004/1004 [==============================] - 22s 22ms/step - loss: 1.7245 - val_loss: 1.7255\n",
            "Epoch 7/10\n",
            "1004/1004 [==============================] - ETA: 0s - loss: 1.6725\n",
            " Generating text after epoch: 7\n",
            "La sottise, l'erreur, le péché de la morte de la morte et de la morte et de la morte et de la morte et de la morte et de la morte et de la morte et de\n",
            "1004/1004 [==============================] - 22s 22ms/step - loss: 1.6725 - val_loss: 1.6853\n",
            "Epoch 8/10\n",
            "1003/1004 [============================>.] - ETA: 0s - loss: 1.6256\n",
            " Generating text after epoch: 8\n",
            "La sottise, l'erreur, le péché de la mortes et les cours de la mortes et les cours de la mortes et les cours de la mortes et les cours de la mortes et\n",
            "1004/1004 [==============================] - 21s 21ms/step - loss: 1.6255 - val_loss: 1.6662\n",
            "Epoch 9/10\n",
            "1002/1004 [============================>.] - ETA: 0s - loss: 1.5829\n",
            " Generating text after epoch: 9\n",
            "La sottise, l'erreur, le péché de la mort de la mort de la mort de la mort de la mort de la mort de la mort de la mort de la mort de la mort de la mor\n",
            "1004/1004 [==============================] - 22s 22ms/step - loss: 1.5828 - val_loss: 1.6306\n",
            "Epoch 10/10\n",
            "1003/1004 [============================>.] - ETA: 0s - loss: 1.5413\n",
            " Generating text after epoch: 10\n",
            "La sottise, l'erreur, le péché de toute des plus de la morte et le parfume et la troide et le parfum\n",
            "   De ton coeur se parfum de la morte et le parfu\n",
            "1004/1004 [==============================] - 21s 21ms/step - loss: 1.5413 - val_loss: 1.6164\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa38fd29e90>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X, y,\n",
        "          batch_size=128,\n",
        "          epochs=10,\n",
        "          validation_split = 0.2,\n",
        "          callbacks=[LambdaCallback(on_epoch_end=end_epoch_generate)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72FpUqWun0WI"
      },
      "source": [
        "#### Using character embeddings \n",
        "- Instead of using one-hot vectors to represent characters, we will now use character embeddings, which are vectors belonging to the same space. \n",
        "- We will need as many vectors as there is characters. The input of the network will be simpler, since we will just need to indicate to the model which character is in input. \n",
        "- The output does not change: indeed, Keras uses one-hot vectors for the target of the categorical cross-entropy loss. \n",
        "Example: the previous example ```sentences``` would now become:\n",
        "- We need to add a ```Embedding``` layer to the model, with the right input size, and to choose which dimension use for our embeddings. \n",
        "\n",
        "```X = [[0, 2, 0, 1],\n",
        "        [0, 1, 1, 0],\n",
        "        [1, 0, 2, 2],\n",
        "        [2, 2, 0, 0],\n",
        "        [0, 0, 1, 1]]```\n",
        "       \n",
        "```y = [[0, 1, 0],\n",
        "      [0, 0, 1],\n",
        "      [1, 0, 0],\n",
        "      [0, 1, 0],\n",
        "      [1, 0, 0]]```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "myi-eu8cuJxF"
      },
      "outputs": [],
      "source": [
        "X_emb = np.zeros((len(sentences), maximum_seq_length), dtype=np.int)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        X_emb[i, t] = char_indices[char]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_2QkLn7itmIF"
      },
      "outputs": [],
      "source": [
        "model_emb = Sequential()\n",
        "model_emb.add(Embedding(len(chars), 32, input_length = maximum_seq_length))\n",
        "model_emb.add(LSTM(256))\n",
        "model_emb.add(Dense(len(chars)))\n",
        "model_emb.add(Activation('softmax'))\n",
        "\n",
        "model_emb.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "J5l1G2-k9kP3"
      },
      "outputs": [],
      "source": [
        "def generate_next(model, text, num_generated=120):\n",
        "    generated = text\n",
        "    sentence = text[-maximum_seq_length:]\n",
        "    for i in range(num_generated):\n",
        "        x = np.zeros((1, maximum_seq_length))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x[0, t] = char_indices[char]\n",
        "        predictions = model.predict(x, verbose=0)[0]\n",
        "        next_index = np.argmax(predictions)\n",
        "        next_char = indices_char[next_index]\n",
        "        generated += next_char\n",
        "        sentence = sentence[1:] + next_char\n",
        "    return(generated)\n",
        "    \n",
        "def end_epoch_generate(epoch, _):\n",
        "    print('\\n Generating text after epoch: %d' % (epoch+1))\n",
        "    texts_ex = [\"La sottise, l'erreur, le péché\"]\n",
        "    for text in texts_ex:\n",
        "        sample = generate_next(model_emb, text)\n",
        "        print('%s' % (sample))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sK70WaUoltY",
        "outputId": "b1b34397-bac3-49d3-a957-5649a2e69278"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "502/502 [==============================] - ETA: 0s - loss: 1.8807\n",
            " Generating text after epoch: 1\n",
            "La sottise, l'erreur, le péchés de soir de l'ain de l'ain de l'ain de l'ain de l'ain de l'ain de l'ain de l'ain de l'ain de l'ain de l'ain de l'ain de\n",
            "502/502 [==============================] - 16s 31ms/step - loss: 1.8807 - val_loss: 1.8452\n",
            "Epoch 2/10\n",
            "502/502 [==============================] - ETA: 0s - loss: 1.8049\n",
            " Generating text after epoch: 2\n",
            "La sottise, l'erreur, le péché de le pris de les pleurs des chasses des chasses des chasses des chasses des chasses des chasses des chasses des chasse\n",
            "502/502 [==============================] - 16s 31ms/step - loss: 1.8049 - val_loss: 1.7899\n",
            "Epoch 3/10\n",
            "501/502 [============================>.] - ETA: 0s - loss: 1.7432\n",
            " Generating text after epoch: 3\n",
            "La sottise, l'erreur, le péchés de la morte de la morte de la morte de la morte de la morte de la morte de la morte de la morte de la morte de la mort\n",
            "502/502 [==============================] - 16s 32ms/step - loss: 1.7432 - val_loss: 1.7340\n",
            "Epoch 4/10\n",
            "502/502 [==============================] - ETA: 0s - loss: 1.6893\n",
            " Generating text after epoch: 4\n",
            "La sottise, l'erreur, le péchés de la morte et les plus de la morte et les plus de la morte et les plus de la morte et les plus de la morte et les plu\n",
            "502/502 [==============================] - 16s 32ms/step - loss: 1.6893 - val_loss: 1.7050\n",
            "Epoch 5/10\n",
            "500/502 [============================>.] - ETA: 0s - loss: 1.6435\n",
            " Generating text after epoch: 5\n",
            "La sottise, l'erreur, le péchés de la moire,\n",
            "   Et de ce souver de coureur de la moire,\n",
            "   Et de ce souver de coureur de la moire,\n",
            "   Et de ce souver \n",
            "502/502 [==============================] - 16s 31ms/step - loss: 1.6432 - val_loss: 1.6662\n",
            "Epoch 6/10\n",
            "500/502 [============================>.] - ETA: 0s - loss: 1.6011\n",
            " Generating text after epoch: 6\n",
            "La sottise, l'erreur, le péché de la moire et de la moire et de la moire et de la moire et de la moire et de la moire et de la moire et de la moire et\n",
            "502/502 [==============================] - 16s 32ms/step - loss: 1.6010 - val_loss: 1.6351\n",
            "Epoch 7/10\n",
            "501/502 [============================>.] - ETA: 0s - loss: 1.5625\n",
            " Generating text after epoch: 7\n",
            "La sottise, l'erreur, le péché de la morte et de la morte et de la morte et de la morte et de la morte et de la morte et de la morte et de la morte et\n",
            "502/502 [==============================] - 16s 32ms/step - loss: 1.5625 - val_loss: 1.6152\n",
            "Epoch 8/10\n",
            "500/502 [============================>.] - ETA: 0s - loss: 1.5267\n",
            " Generating text after epoch: 8\n",
            "La sottise, l'erreur, le péché de la troire et les plus comme un coeur et le coeur et le coeur et le coeur et le coeur et le coeur et le coeur et le c\n",
            "502/502 [==============================] - 16s 33ms/step - loss: 1.5267 - val_loss: 1.5942\n",
            "Epoch 9/10\n",
            "500/502 [============================>.] - ETA: 0s - loss: 1.4932\n",
            " Generating text after epoch: 9\n",
            "La sottise, l'erreur, le péché de la coureur de la coureur de la coureur de la coureur de la coureur de la coureur de la coureur de la coureur de la c\n",
            "502/502 [==============================] - 16s 33ms/step - loss: 1.4935 - val_loss: 1.5805\n",
            "Epoch 10/10\n",
            "502/502 [==============================] - ETA: 0s - loss: 1.4637\n",
            " Generating text after epoch: 10\n",
            "La sottise, l'erreur, le péché de la morte et le fort de la morte et le fort de la morte et le fort de la morte et le fort de la morte et le fort de l\n",
            "502/502 [==============================] - 16s 31ms/step - loss: 1.4637 - val_loss: 1.5610\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa363cdd2d0>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_emb.fit(X_emb, y,\n",
        "          batch_size=128,\n",
        "          epochs=10,\n",
        "          validation_split = 0.2,\n",
        "          callbacks=[LambdaCallback(on_epoch_end=end_epoch_generate)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ATJnocork5a"
      },
      "source": [
        "#### Sampling with our model\n",
        "- Now, instead of simply selecting the most probable next character, we would like to be able to draw a sample from the distribution output by the model.\n",
        "- To better control the generation, we would like to use the argument ```temperature```, to smooth the distribution.\n",
        "- We will use the ```multinomial``` function from the ```random``` package to draw samples.\n",
        "- We integrate this into a function ```generate_sample``` that is almost exactly like ```generate_next```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "psIfwKiLol3W"
      },
      "outputs": [],
      "source": [
        "def reweight(predictions, temperature):\n",
        "    predictions = np.asarray(predictions).astype('float64')\n",
        "    log_predictions = np.log(predictions) / temperature\n",
        "    predictions = np.exp(log_predictions)\n",
        "    predictions = predictions / np.sum(predictions)\n",
        "    return predictions   \n",
        "\n",
        "def sample(predictions, temperature):\n",
        "    predictions = reweight(predictions, temperature)\n",
        "    probas = np.random.multinomial(1, predictions, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "def generate_sample(model, text, num_generated=120, temperature=1.0):\n",
        "    generated = text\n",
        "    sentence = text[-maximum_seq_length:]\n",
        "    for i in range(num_generated):\n",
        "        x = np.zeros((1, maximum_seq_length))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x[0, t] = char_indices[char]\n",
        "        predictions = model.predict(x, verbose=0)[0]\n",
        "        next_index = sample(predictions, temperature)\n",
        "        next_char = indices_char[next_index]\n",
        "        generated += next_char\n",
        "        sentence = sentence[1:] + next_char\n",
        "    return(generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy5z2vW1kTHy",
        "outputId": "a4f3d97c-ec16-4e4f-ac0b-ab269eace38a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La sottise, l'erreur, le péché la nuit mon jaye et ses bauteurs serres aux; brassiquon,\n",
            "   Tu connus qui fit le singulier les cepuneux pareurs,\n",
            "   Des\n"
          ]
        }
      ],
      "source": [
        "print(generate_sample(model_emb, text_ex, temperature = 0.7))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "U_wD4v8Qy-nG"
      },
      "outputs": [],
      "source": [
        "def sample_top_k(predictions, temperature, k):\n",
        "    predictions = np.asarray(predictions).astype('float64')\n",
        "    log_predictions = np.log(predictions) / temperature\n",
        "    indices_to_remove = log_predictions.argsort()[:k]\n",
        "    log_predictions[indices_to_remove] = -float('Inf')\n",
        "    predictions = np.exp(log_predictions)\n",
        "    predictions = predictions / np.sum(predictions)\n",
        "    probas = np.random.multinomial(1, predictions, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def generate_sample_top_k(model, text, num_generated=120, temperature=1.0, k=10):\n",
        "    generated = text\n",
        "    sentence = text[-maximum_seq_length:]\n",
        "    for i in range(num_generated):\n",
        "        x = np.zeros((1, maximum_seq_length))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x[0, t] = char_indices[char]\n",
        "        predictions = model.predict(x, verbose=0)[0]\n",
        "        next_index = sample_top_k(predictions, temperature, k)\n",
        "        next_char = indices_char[next_index]\n",
        "        generated += next_char\n",
        "        sentence = sentence[1:] + next_char\n",
        "    return(generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hXKdtloy-rm",
        "outputId": "e1c842a9-ac19-4c79-f41f-019b35cc5f3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La sottise, l'erreur, le péché sur le flasse et ses pleurs.\n",
            " \n",
            "   C'est l'on ma moile, harmire et sans soupire\n",
            "   Sous le lourd tien de la noir et les \n"
          ]
        }
      ],
      "source": [
        "print(generate_sample_top_k(model_emb, text_ex, temperature = 0.7, k=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "qDWCo3GczC9j"
      },
      "outputs": [],
      "source": [
        "def sample_top_p(predictions, temperature, p):\n",
        "    predictions = np.asarray(predictions).astype('float64')\n",
        "    log_predictions = np.log(predictions) / temperature\n",
        "    predictions = np.exp(log_predictions)\n",
        "    predictions = predictions / np.sum(predictions)\n",
        "    \n",
        "    cum_prob = 0.0\n",
        "    incr = 0\n",
        "    indices = predictions.argsort()\n",
        "    probs = predictions[indices]\n",
        "    while cum_prob < p:\n",
        "        cum_prob += probs[incr]\n",
        "        incr += 1\n",
        "    indices_to_remove = indices[incr:]\n",
        "    \n",
        "    log_predictions[indices_to_remove] = -float('Inf')\n",
        "    predictions = np.exp(log_predictions)\n",
        "    predictions = predictions / np.sum(predictions)\n",
        "    probas = np.random.multinomial(1, predictions, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def generate_sample_top_p(model, text, num_generated=60, temperature=1.0, p=0.9):\n",
        "    generated = text\n",
        "    sentence = text[-maximum_seq_length:]\n",
        "    for i in range(num_generated):\n",
        "        x = np.zeros((1, maximum_seq_length))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x[0, t] = char_indices[char]\n",
        "        predictions = model.predict(x, verbose=0)[0]\n",
        "        next_index = sample_top_p(predictions, temperature, p)\n",
        "        next_char = indices_char[next_index]\n",
        "        generated += next_char\n",
        "        sentence = sentence[1:] + next_char\n",
        "    return(generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UanOyXX8zDAQ",
        "outputId": "6162dee9-53db-4309-d124-4230807c122c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "la sottise, l'erreur, le péchérait autome au sentique éternelle,\n",
            "   Des rends les souirs s\n"
          ]
        }
      ],
      "source": [
        "print(generate_sample_top_p(model_emb, text_ex.lower(), temperature = 0.7, p = 0.6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa1z4yJzrvMp"
      },
      "source": [
        "#### Generate text with the beam algorithm\n",
        "- We need to loop for each character we want to generate, keeping track of the best ```beam_size``` sequences at the most. \n",
        "- Besides keeping track of past generated character for each of these ```beam_size``` sequences, we need to keep track of their log-probability.\n",
        "- This is done by, at each loop, keeping the ```beam_size```best predictions for each of the ```beam_size``` sequences, computing the log-probabilities of the newly formed (```beam_size```)$^2$ , and keeping the overall ```beam_size``` best new sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "5FTEE4Aqz55B"
      },
      "outputs": [],
      "source": [
        "def generate_beam(model, text, beam_size=16, num_generated=128):\n",
        "    generated = text\n",
        "    sentence = text[-maximum_seq_length:]\n",
        "    # Initialization of the beam with log-probabilities for the sequence\n",
        "    current_beam = [(0, [], sentence)]\n",
        "\n",
        "    for l in range(num_generated):\n",
        "        all_beams = []\n",
        "        for prob, current_preds, current_input in current_beam:\n",
        "            x = np.zeros((1, maximum_seq_length))\n",
        "            for t, char in enumerate(current_input):\n",
        "                x[0, t] = char_indices[char]\n",
        "            prediction = model.predict(x=x)[0]\n",
        "            possible_next_chars = prediction.argsort()[-beam_size:][::-1]\n",
        "            all_beams += [\n",
        "                (prob + np.log(prediction[next_index]),\n",
        "                 current_preds + [next_index],\n",
        "                 current_input[1:] + indices_char[next_index]   \n",
        "                )\n",
        "                for next_index in possible_next_chars]\n",
        "\n",
        "        current_beam = sorted(all_beams)[-beam_size:]\n",
        "\n",
        "    return text + ''.join([indices_char[idx] for idx in current_preds])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk0YnN8mz6Cs",
        "outputId": "036b22ad-49f1-4d96-c194-d48b83503d1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La sottise, l'erreur, le péché,\n",
            "   Comme un coeur d'un coeur et le soir,\n",
            "   Comme un coeur d'un coeur et le soir,\n",
            "   Comme un coeur d'un coeur et le soir,\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "print(generate_beam(model_emb, text_ex))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CharacterBasedGeneration.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
