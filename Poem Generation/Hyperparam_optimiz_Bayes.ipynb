{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le code est mis de coté pour cause du temps de calcul. Il s'intégre dans la partie train du modèle en remplacement des cellules de la partie Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIGNE TO INSTALL THE PACKAGE\n",
    "#pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by definying our function, bounds, and instanciating an optimization object.\n",
    "def black_box_function(EMBEDDING_SIZE, SEED, HIDDEN_SIZE, N_LAYERS, LR, EPOCHS):\n",
    "\n",
    "    torch.manual_seed(SEED) # set the random seed manually for reproducibility.\n",
    "    pass\n",
    "    \n",
    "    # Display some figures for model dimensioning\n",
    "    n_words = len(corpus.dictionary.word2idx)\n",
    "    n_phons = len(corpus.dictionary_phon.word2idx)\n",
    "    print(f\"Number of words in the corpus: {n_words}.\")\n",
    "    print(f\"Number of phonems in the corpus: {n_phons}.\")\n",
    "    print(f\"Ratio [phonems / words]: {n_phons/n_words:.2f}.\")\n",
    "    log_words = np.log(n_words)\n",
    "    log_phons = np.log(n_phons)\n",
    "    print(f\"Ratio [log(phonems) / log(words)]: {log_phons/log_words:.2f}.\")\n",
    "    \n",
    "    # Define weights for the Cross Entropy Loss\n",
    "    # The more frequent the word is, the less weighted \n",
    "    def get_weight(corpus):\n",
    "        n_eos = int((corpus.train == 0).sum()) + int((corpus.test == 0).sum()) \\\n",
    "        + int((corpus.valid == 0).sum())\n",
    "        weight = [n_eos]\n",
    "        weight += sorted(list(corpus.dictionary.frequency.values()), reverse=True)\n",
    "        weight = 1 / np.log(np.array(weight))\n",
    "        return torch.tensor(weight, dtype=torch.float).to(device)\n",
    "    \n",
    "    # Loss selection\n",
    "    WEIGHT = False\n",
    "    if WEIGHT:\n",
    "        weight = get_weight(corpus)\n",
    "        criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Model selection\n",
    "    model = LSTMModel(ntoken=N_TOKENS, ninp=EMBEDDING_SIZE,\n",
    "                    nhid=HIDDEN_SIZE, nlayers=N_LAYERS).to(device)\n",
    "\n",
    "\n",
    "    n_params = torch.nn.utils.parameters_to_vector(model.parameters()).numel()\n",
    "  \n",
    "    \n",
    "    \n",
    "    OPTIMIZER = 'sgd'   # Optimizer (SGD or Adam)\n",
    "    WDECAY = 0          # Weight decay\n",
    "    CLIP = 0.25         # Gradient clippin\n",
    "    \n",
    "    if OPTIMIZER == 'sgd':\n",
    "        optim = torch.optim.SGD(model.parameters(), lr=LR, weight_decay=WDECAY)\n",
    "    elif OPTIMIZER == 'adam':\n",
    "        optim = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WDECAY)\n",
    "    else:\n",
    "        optim = None\n",
    "\n",
    "    if OPTIMIZER in ['sgd', 'adam']:\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=1, gamma=0.7)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "\n",
    "    MODEL_NAME = 'model_classic'\n",
    "    SAVE_PATH = os.path.join(CHECKPOINT_PATH, '{}.pt'.format(MODEL_NAME))\n",
    "    LOAD_EXISTING_MODEL = False  # Change to False if you want to start from scratch\n",
    "                                # ⚠ If False, any existing model with the same name\n",
    "                                # will be overwritten ⚠\n",
    "    TENSORBOARD = True           # Monitor the training\n",
    "\n",
    "\n",
    "    if LOAD_EXISTING_MODEL:\n",
    "        try:\n",
    "            with open(SAVE_PATH, 'rb') as f:\n",
    "                model = torch.load(f)\n",
    "                # after load the rnn params are not a continuous chunk of memory\n",
    "                # this makes them a continuous chunk, and will speed up forward pass\n",
    "                model.rnn.flatten_parameters()\n",
    "            print(\"Successfully loaded model from {}\".format(SAVE_PATH))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    best_val_loss = None\n",
    "\n",
    "    # Loop over epochs.\n",
    "    # At any point you can hit Ctrl + C to break out of training early.\n",
    "    try:\n",
    "\n",
    "        train_data, val_data, test_data = get_train_valid_test_data(model.phon,\n",
    "                                                                    ordered=True)\n",
    "        # Initialize tensorboard\n",
    "        if TENSORBOARD:\n",
    "            iter = 1\n",
    "            comment=f'model={MODEL_NAME}_IN={INPUT_SIZE}_INPHON={INPUT_SIZE_PHON}'\n",
    "            writer = SummaryWriter(comment=comment)\n",
    "            for name, weight in model.named_parameters():\n",
    "                writer.add_histogram(name, weight, 0)\n",
    "\n",
    "        for epoch in range(1, EPOCHS+1):\n",
    "            epoch_start_time = time.time()\n",
    "            train()\n",
    "            val_loss = min(20, evaluate(val_data))\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            # Monitoring\n",
    "            if TENSORBOARD:\n",
    "                for name, weight in model.named_parameters():\n",
    "                    writer.add_histogram(name, weight, epoch)\n",
    "                    writer.add_histogram(f'{name}.grad', weight.grad, epoch)\n",
    "                writer.add_scalar('Loss_Val', val_loss, epoch)\n",
    "\n",
    "            # Save the model if the validation loss is the best we've seen so far.\n",
    "            if not best_val_loss or val_loss < best_val_loss:\n",
    "                with open(SAVE_PATH, 'wb') as f:\n",
    "                    torch.save(model, f)\n",
    "                print(\"Successfully saved model at {}\\n\".format(SAVE_PATH))\n",
    "                best_val_loss = val_loss\n",
    "            else:\n",
    "                if scheduler is None:\n",
    "                    lr /= 4.0\n",
    "\n",
    "        if TENSORBOARD:\n",
    "            writer.flush()\n",
    "            writer.close()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('-' * 89)\n",
    "        print('Exiting from training early')\n",
    "        if TENSORBOARD:\n",
    "            writer.flush()\n",
    "            writer.close()\n",
    "\n",
    "    \n",
    "    test_loss = evaluate(test_data)\n",
    "\n",
    "\n",
    "    return -test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounded region of parameter space\n",
    "pbounds = {'SEED': (30, 50), 'EMBEDDING_SIZE': (250, 1250), 'HIDDEN_SIZE': (500, 2000), 'N_LAYERS': (10,30) , 'LR' : (0,20), 'LR' : (5,10)  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    f=black_box_function,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.maximize(\n",
    "    init_points=2,\n",
    "    n_iter=5,\n",
    "    acq=\"ei\", \n",
    "    xi=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimizer.max)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "101b50cc4a41aaa5ed2a0bdfdf92b82b0896a3b48cbaa7c11312209135d80007"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('arsene')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
